{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2QU2YNJVzuZ+jjVogWXxf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elangbijak4/Time-Series-Research/blob/main/Modifikasi2_Testing_TXMIXER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penambahan Layer LSTM untuk menangkap pola jangka panjang. Penambahan ini dilakukan pada Modifikasi1 TSMIXER."
      ],
      "metadata": {
        "id": "69PdOEdZXCTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "91e5sRwsYNn0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def res_block_with_lstm(inputs, norm_type, activation, dropout, ff_dim, lstm_units):\n",
        "    norm = (\n",
        "        layers.LayerNormalization\n",
        "        if norm_type == 'L'\n",
        "        else layers.BatchNormalization\n",
        "    )\n",
        "\n",
        "    # Temporal Linear with Attention\n",
        "    x = norm(axis=[-2, -1])(inputs)\n",
        "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
        "    x = layers.MultiHeadAttention(num_heads=4, key_dim=x.shape[-1])(x, x)\n",
        "    x = layers.Dense(x.shape[-1], activation=activation)(x)\n",
        "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Input Length, Channel]\n",
        "    x = layers.SpatialDropout1D(dropout)(x)\n",
        "\n",
        "    # Adding LSTM layer\n",
        "    x = layers.LSTM(lstm_units, return_sequences=True)(x)\n",
        "\n",
        "    # Project LSTM output to match input dimension\n",
        "    x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]\n",
        "\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feature Linear\n",
        "    x = norm(axis=[-2, -1])(res)\n",
        "    x = layers.Dense(ff_dim, activation=activation)(\n",
        "        x\n",
        "    )  # [Batch, Input Length, FF_Dim]\n",
        "    x = layers.SpatialDropout1D(dropout)(x)\n",
        "    x = layers.Dense(inputs.shape[-1])(x)  # [Batch, Input Length, Channel]\n",
        "    x = layers.SpatialDropout1D(dropout)(x)\n",
        "    return x + res\n",
        "\n",
        "# Build enhanced model with LSTM\n",
        "def build_enhanced_model_with_lstm(input_shape, pred_len, norm_type, activation, n_block, dropout, ff_dim, lstm_units, target_slice):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = inputs  # [Batch, Input Length, Channel]\n",
        "    for _ in range(n_block):\n",
        "        x = res_block_with_lstm(x, norm_type, activation, dropout, ff_dim, lstm_units)\n",
        "\n",
        "    if target_slice:\n",
        "        x = x[:, :, target_slice]\n",
        "\n",
        "    x = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Channel, Input Length]\n",
        "    x = layers.Dense(pred_len)(x)  # [Batch, Channel, Output Length]\n",
        "    outputs = tf.transpose(x, perm=[0, 2, 1])  # [Batch, Output Length, Channel]\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "B6ZMW2G6YUwQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Vsp_anjpXm9P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "batch_size = 64\n",
        "input_length = 30\n",
        "num_features = 10\n",
        "pred_length = 5\n",
        "\n",
        "# Generate dummy data\n",
        "x_train = np.random.rand(batch_size, input_length, num_features)\n",
        "y_train = np.random.rand(batch_size, pred_length, num_features)\n",
        "\n",
        "# Define model parameters\n",
        "input_shape = (input_length, num_features)\n",
        "norm_type = 'L'\n",
        "activation = 'relu'\n",
        "n_block = 4\n",
        "dropout = 0.1\n",
        "ff_dim = 64\n",
        "lstm_units = 50  # Number of LSTM units\n",
        "target_slice = None\n",
        "\n",
        "# Build model\n",
        "model = build_enhanced_model_with_lstm(input_shape, pred_length, norm_type, activation, n_block, dropout, ff_dim, lstm_units, target_slice)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n",
        "\n",
        "# Generate new data for inference\n",
        "x_new = np.random.rand(batch_size, input_length, num_features)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(x_new)\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Input shape: {x_new.shape}\")\n",
        "print(f\"Predicted output shape: {y_pred.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaKtKysCXb-3",
        "outputId": "2e5b08a9-e4ca-4085-94b1-77134e375255"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 17s 17s/step - loss: 2.0669\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.9133\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.6820\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.4363\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.3923\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.2415\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 1.1116\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.9965\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.8956\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.8821\n",
            "2/2 [==============================] - 2s 27ms/step\n",
            "Input shape: (64, 30, 10)\n",
            "Predicted output shape: (64, 5, 10)\n"
          ]
        }
      ]
    }
  ]
}